# pipeline/main_extraction.py
# ä¸»ç¨‹åºï¼šå…¨é‡è¯­æ–™åº“å¹¶è¡Œæå–æ‰§è¡Œå™¨
# åŠŸèƒ½ï¼šæ‰«ææ‰€æœ‰è¯­æ–™ -> æ£€æŸ¥å·²å¤„ç†åˆ—è¡¨ -> å¤šçº¿ç¨‹è°ƒç”¨æå–å™¨ -> å®æ—¶ä¿å­˜ç»“æœ

import os
# Ensure HF mirror is set if needed, though usually handled by env vars
# os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com' 

import json
import logging
import concurrent.futures
from tqdm import tqdm
from pathlib import Path
from typing import Dict, Any, Optional

# å¯¼å…¥æˆ‘ä»¬çš„å·¥ç¨‹æ¨¡å— (ä½œä¸ºåŒ…å¯¼å…¥)
# ç¡®ä¿åœ¨ pipeline/ ç›®å½•ä¸‹æœ‰ __init__.py (è™½ç„¶ Python 3.3+ ä¸éœ€è¦ï¼Œä½†æ¨èåŠ ä¸Š)
try:
    from mea_kg_builder import config
    from mea_kg_builder.extractor import global_extractor
except ImportError:
    # Fallback for running directly without package context
    import sys
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from mea_kg_builder import config
    from mea_kg_builder.extractor import global_extractor

# --- é…ç½®æ—¥å¿— ---
# è®¾ç½®ä¸º INFO çº§åˆ«ï¼Œæ—¢èƒ½çœ‹åˆ°è¿›åº¦ï¼Œåˆä¸ä¼šè¢«å¤ªå¤šç»†èŠ‚æ·¹æ²¡
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("extraction.log", mode='a', encoding='utf-8'),  # ä¿å­˜æ—¥å¿—åˆ°æ–‡ä»¶
        logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
    ]
)
logger = logging.getLogger(__name__)


# --- è¾…åŠ©å‡½æ•° ---

def get_processed_files() -> set:
    """
    è¯»å–ç°æœ‰çš„ç»“æœæ–‡ä»¶ï¼Œè·å–æ‰€æœ‰å·²æˆåŠŸå¤„ç†çš„æ–‡ä»¶åé›†åˆã€‚
    ç”¨äºå®ç°æ–­ç‚¹ç»­ä¼ ã€‚
    """
    processed = set()
    output_path = config.EXTRACTION_RESULTS_PATH
    
    if os.path.exists(output_path):
        logger.info(f"å‘ç°ç°æœ‰ç»“æœæ–‡ä»¶: {output_path}ï¼Œæ­£åœ¨æ‰«æå·²å¤„ç†è®°å½•...")
        try:
            with open(output_path, 'r', encoding='utf-8') as f:
                for line in f:
                    try:
                        if not line.strip(): continue
                        record = json.loads(line)
                        # å‡è®¾æ¯ä¸ªè®°å½•éƒ½æœ‰ 'filename' å­—æ®µ
                        if 'filename' in record:
                            processed.add(record['filename'])
                    except json.JSONDecodeError:
                        continue
        except Exception as e:
            logger.error(f"è¯»å–ç°æœ‰ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
            
    logger.info(f"å·²æ‰¾åˆ° {len(processed)} ä¸ªå·²å¤„ç†æ–‡ä»¶ã€‚")
    return processed


def process_single_file(file_info: Dict[str, Any]) -> Optional[str]:
    """
    å¤„ç†å•ä¸ªæ–‡ä»¶çš„æ ¸å¿ƒå·¥ä½œå‡½æ•°ã€‚
    ä¼šè¢«çº¿ç¨‹æ± è°ƒç”¨ã€‚
    """
    file_path = file_info['path']
    filename = file_info['filename']
    source_type = file_info['source_type']

    try:
        # 1. è¯»å–æ–‡æœ¬
        with open(file_path, 'r', encoding='utf-8') as f:
            text = f.read()

        # 2. è°ƒç”¨æ ¸å¿ƒå¼•æ“è¿›è¡Œæå– (åŒ…å« Phase 1 & Phase 2)
        # global_extractor å†…éƒ¨å·²ç»å°è£…äº†é‡è¯•æœºåˆ¶
        triplets = global_extractor.extract(text)

        # 3. æ ¼å¼åŒ–ç»“æœè®°å½•
        if triplets:
            result_record = {
                "filename": filename,
                "source_type": source_type,
                # å°† Pydantic å¯¹è±¡åˆ—è¡¨è½¬æ¢ä¸ºæ™®é€šå­—å…¸åˆ—è¡¨
                "triplets": [t.model_dump() for t in triplets]
            }
            # è¿”å› JSON å­—ç¬¦ä¸²ä»¥ä¾¿å†™å…¥
            return json.dumps(result_record, ensure_ascii=False)
        else:
            # å³ä½¿æ²¡æœ‰æå–åˆ°ä¸‰å…ƒç»„ï¼Œä¹Ÿè®°å½•ä¸€æ¡ç©ºç»“æœï¼Œé¿å…é‡å¤å¤„ç†
            # (å¯é€‰) ä¹Ÿå¯ä»¥é€‰æ‹©ä¸è®°å½•ï¼Œä½†è¿™ä¼šå¯¼è‡´ä¸‹æ¬¡è¿è¡Œé‡æ–°å¤„ç†è¯¥æ–‡ä»¶
            return json.dumps({"filename": filename, "source_type": source_type, "triplets": []}, ensure_ascii=False)

    except Exception as e:
        logger.error(f"å¤„ç†æ–‡ä»¶å¤±è´¥ [{filename}]: {e}")
        return None


# --- ä¸»æ‰§è¡Œæµç¨‹ ---

def main():
    logger.info("=== å¼€å§‹å…¨é‡è¯­æ–™åº“çŸ¥è¯†æå–ä»»åŠ¡ ===")

    # 1. æ‰«ææ‰€æœ‰å¾…å¤„ç†æ–‡ä»¶
    corpus_root = Path(config.CORPUS_DIR)
    all_files = []
    
    # éå† academic, news, web ä¸‰ä¸ªå­ç›®å½•
    # è¿™äº›ç›®å½•åå¿…é¡»ä¸ corpus/preprocess.py ä¸­ç”Ÿæˆçš„ä¸€è‡´
    for sub_dir in ['academic', 'news', 'web']:
        dir_path = corpus_root / sub_dir
        if dir_path.exists():
            # é€’å½’æŸ¥æ‰¾æ‰€æœ‰ .txt æ–‡ä»¶
            files_in_dir = list(dir_path.glob('*.txt'))
            for file_path in files_in_dir:
                all_files.append({
                    'path': str(file_path),
                    'filename': file_path.name,
                    'source_type': sub_dir
                })
            logger.info(f"ç›®å½• '{sub_dir}' å‘ç° {len(files_in_dir)} ä¸ªæ–‡ä»¶ã€‚")
        else:
            logger.warning(f"è­¦å‘Š: è¯­æ–™åº“å­ç›®å½•ä¸å­˜åœ¨: {dir_path}")

    logger.info(f"æ‰«æåˆ°è¯­æ–™åº“æ–‡ä»¶æ€»æ•°: {len(all_files)}")

    if not all_files:
        logger.error("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•è¯­æ–™æ–‡ä»¶ã€‚è¯·å…ˆè¿è¡Œ corpus/preprocess.pyã€‚")
        return

    # 2. è¿‡æ»¤æ‰å·²å¤„ç†çš„æ–‡ä»¶ (æ–­ç‚¹ç»­ä¼ )
    processed_files = get_processed_files()
    tasks_to_do = [f for f in all_files if f['filename'] not in processed_files]
    
    logger.info(f"å‰©ä½™å¾…å¤„ç†æ–‡ä»¶æ•°: {len(tasks_to_do)}")

    if not tasks_to_do:
        logger.info("æ‰€æœ‰æ–‡ä»¶å‡å·²å¤„ç†å®Œæ¯•ï¼ğŸ‰")
        return

    # 3. å¹¶è¡Œæ‰§è¡Œæå–ä»»åŠ¡
    # ä½¿ç”¨ ThreadPoolExecutor å› ä¸ºä¸»è¦ç“¶é¢ˆæ˜¯ I/O (ç½‘ç»œ/APIè°ƒç”¨)
    # å¦‚æœæ˜¯æœ¬åœ°æ¨¡å‹ï¼ŒMAX_WORKERS å»ºè®®è®¾ä¸º 1ï¼Œå¦åˆ™æ˜¾å­˜å®¹æ˜“çˆ†
    max_workers = config.MAX_WORKERS
    logger.info(f"å¯åŠ¨çº¿ç¨‹æ± ï¼Œå¹¶å‘æ•°: {max_workers}")

    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(config.EXTRACTION_RESULTS_PATH), exist_ok=True)

    # æ‰“å¼€è¾“å‡ºæ–‡ä»¶ (è¿½åŠ æ¨¡å¼ 'a')
    with open(config.EXTRACTION_RESULTS_PATH, 'a', encoding='utf-8') as f_out:

        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            # ä½¿ç”¨ future_to_file å­—å…¸æ¥è¿½è¸ªæ¯ä¸ªä»»åŠ¡å¯¹åº”çš„æ–‡ä»¶
            future_to_file = {
                executor.submit(process_single_file, file_info): file_info['filename'] 
                for file_info in tasks_to_do
            }

            # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
            progress_bar = tqdm(total=len(tasks_to_do), desc="Processing Corpus", dynamic_ncols=True)

            for future in concurrent.futures.as_completed(future_to_file):
                filename = future_to_file[future]
                try:
                    result_json_str = future.result()
                    if result_json_str:
                        # å®æ—¶å†™å…¥ç»“æœåˆ°æ–‡ä»¶
                        f_out.write(result_json_str + '\n')
                        f_out.flush()  # ç¡®ä¿ç«‹å³å†™å…¥ç¡¬ç›˜ï¼Œé˜²æ­¢æ•°æ®ä¸¢å¤±

                except Exception as e:
                    logger.error(f"ä»»åŠ¡æ‰§è¡Œå¼‚å¸¸ [{filename}]: {e}")

                finally:
                    progress_bar.update(1)

            progress_bar.close()

    logger.info("=== å…¨é‡æå–ä»»åŠ¡å®Œæˆ ===")
    logger.info(f"ç»“æœå·²ä¿å­˜è‡³: {config.EXTRACTION_RESULTS_PATH}")


if __name__ == '__main__':
    # å†æ¬¡æé†’ï¼šå¦‚æœåœ¨æœ¬åœ°è·‘ï¼ŒåŠ¡å¿…ç¡®è®¤æ˜¾å­˜è¶³å¤Ÿ
    print(f"å½“å‰é…ç½®å¹¶å‘æ•°: {config.MAX_WORKERS}")
    print("å¦‚æœä½¿ç”¨æœ¬åœ° Ollama ä¸”æ˜¾å­˜è¾ƒå°(<24G)ï¼Œè¯·ç¡®ä¿ mea_kg_builder/config.py ä¸­ MAX_WORKERS = 1")
    
    # å¼€å§‹è¿è¡Œ
    main()
